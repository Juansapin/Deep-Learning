{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f2e8ecbc","cell_type":"markdown","source":"# Caso de Estudio: Predicción de Éxito en Campañas de Telemercadeo con Deep Learning\n\n**Autor:** Luis E. Seijas\n\n**Curso:** Deep Learning para Negocios\n\n---\n\n### 1. Descripción del Problema\n\nEn este proyecto, actuarás como Científico de Datos para una institución financiera. El banco necesita optimizar sus campañas de **telemercadeo** para ofrecer depósitos a plazo, un producto financiero clave. Cada llamada tiene un costo, y llamar a clientes que no están interesados no solo es ineficiente, sino que también puede generar una mala experiencia para el cliente.\n\n**El objetivo principal es:**\n\n> Construir un modelo de clasificación basado en redes neuronales profundas que pueda predecir con alta precisión si un cliente suscribirá (`'sí'`) o no (`'no'`) un depósito a plazo después de ser contactado.\n\nUn modelo exitoso permitirá al banco:\n*  **Focalizar los esfuerzos:** Concentrar las llamadas en los clientes con mayor probabilidad de conversión.\n*  **Reducir costos:** Minimizar el número de llamadas innecesarias.\n*  **Aumentar la tasa de éxito:** Mejorar el retorno de inversión (ROI) de las campañas de marketing.\n\n**Es importante considerar:** Los datos son una mezcla de información demográfica, historial bancario y métricas de la campaña. Tu tarea es procesar estos datos, diseñar una arquitectura de red neuronal efectiva y, finalmente, traducir tus resultados en recomendaciones de negocio accionables.","metadata":{}},{"id":"d6ca6a51-4b99-4309-8f55-d077961b5d0c","cell_type":"code","source":"import tensorflow as tf\n\nprint(f\"Versión de TensorFlow: {tf.__version__}\")\n\ngpus = tf.config.list_physical_devices('GPU')\n\nif gpus:\n    try:\n        # Esta configuración es una buena práctica para evitar que TensorFlow\n        # reserve toda la memoria de la GPU desde el inicio.\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        \n        print(f\"✅ GPU(s) detectada(s): {len(gpus)}. La configuración se aplicó correctamente.\")\n        print(\"TensorFlow utilizará la GPU para el entrenamiento.\")\n    except RuntimeError as e:\n        # Este bloque se ejecuta si la GPU ya fue inicializada.\n        print(f\"⚠️ Error al configurar la GPU: {e}\")\n        print(\"La GPU ya estaba inicializada. Si necesitas cambiar la configuración, reinicia el kernel.\")\nelse:\n    print(\"❌ No se detectó ninguna GPU. El entrenamiento se realizará en la CPU.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"83e6212c","cell_type":"markdown","source":"#### 1.1. Carga de datos \n\n","metadata":{}},{"id":"4d6b20f4","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 1: Configuración del entorno y carga de datos.\n\nEn este paso, instalaremos la librería recomendada para acceder al repositorio\nde la UCI, importaremos las herramientas necesarias y cargaremos los datos.\n\"\"\"\n\n# 1. Instalación de la librería de UCI (si no está instalada)\n# Descomenta la siguiente línea y ejecútala si es la primera vez que usas esta librería.\n# !pip install ucimlrepo\n\n# 2. Librerías para manipulación y análisis de datos\nimport pandas as pd\nimport numpy as np\nfrom ucimlrepo import fetch_ucirepo \n\n# 3. Librerías para visualización de datos\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configuraciones para una mejor visualización\nsns.set_style('whitegrid')\nplt.style.use('fivethirtyeight')\n\n# 4. Carga del conjunto de datos usando el ID del repositorio\n# ID 222 corresponde al dataset \"Bank Marketing\"\n# Moro, S., Rita, P., & Cortez, P. (2014). Bank Marketing [Dataset]. \n# UCI Machine Learning Repository. https://doi.org/10.24432/C5K306.\nbank_marketing = fetch_ucirepo(id=222) \n\n# Extracción de los datos en DataFrames de pandas\n# X contiene las variables predictoras (features)\nX = bank_marketing.data.features \n# y contiene la variable objetivo (target)\ny = bank_marketing.data.targets \n\n# Para facilitar el análisis exploratorio, uniremos X e y en un solo DataFrame\ndf = pd.concat([X, y], axis=1)\n\nprint(\"¡Datos cargados exitosamente usando ucimlrepo!\")\nprint(f\"El conjunto de datos tiene {df.shape[0]} filas y {df.shape[1]} columnas.\")\n\n# 5. Primera visualización de los datos\n# Usamos .head() para mostrar las primeras 5 filas.\nprint(\"\\nPrimeras 5 filas del conjunto de datos:\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ccda22e6","cell_type":"markdown","source":"### 2. Análisis Exploratorio de Datos (EDA)\n\nAntes de proceder con la construcción de un modelo, es imperativo realizar un Análisis Exploratorio de Datos (EDA). Este proceso consiste en la investigación sistemática del conjunto de datos para identificar patrones, detectar anomalías, validar supuestos y extraer conclusiones iniciales mediante el uso de estadísticas descriptivas y técnicas de visualización.\n\nLos objetivos de esta sección son:\n1.  **Analizar la estructura y tipos de datos:** Identificar las variables, sus tipos (numéricas, categóricas) y evaluar la integridad de los datos.\n2.  **Examinar la variable objetivo:** Cuantificar la distribución de la variable de salida para identificar posibles sesgos, como el desbalance de clases.\n3.  **Visualizar relaciones y distribuciones:** Investigar la relación entre las variables predictoras y la variable objetivo.","metadata":{}},{"id":"fa344c0a","cell_type":"code","source":"# La librería nos da acceso directo a información valiosa sobre los datos.\n\n# Mostramos la metadata del dataset\nprint(\"------ METADATA DEL DATASET ------\")\nprint(bank_marketing.metadata)\n\n# Mostramos la descripción de cada variable\nprint(\"\\n------ INFORMACIÓN DE LAS VARIABLES ------\")\n# pd.set_option('display.max_rows', None) # Descomentar para ver todas las variables\nprint(bank_marketing.variables)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"508b64d5","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 2.1: Inspección inicial de la calidad de los datos.\n\nVerificación de tipos de datos por columna y presencia de valores nulos.\n\"\"\"\n\n# El método .info() proporciona un resumen conciso del DataFrame,\n# incluyendo el tipo de dato de cada columna y el conteo de valores no nulos.\nprint(\"------ Resumen del DataFrame ------\")\ndf.info()\n\n# Se realiza una comprobación explícita de la suma de valores nulos.\nprint(\"\\n------ Conteo de Valores Nulos por Columna ------\")\nprint(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c95e141b","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 2.2: Análisis de la variable objetivo 'y'.\n\nEste bloque responde a la Pregunta 1: ¿Cuál es la proporción de clientes que\nsuscribieron el depósito ('yes') frente a los que no ('no')?\n\"\"\"\n\n# 1. Conteo de frecuencias absolutas para cada clase.\ntarget_counts = df['y'].value_counts()\nprint(\"------ Conteo de Clases en la Variable Objetivo ------\")\nprint(target_counts)\n\n# 2. Cálculo de las frecuencias relativas (porcentajes).\ntarget_percentage = df['y'].value_counts(normalize=True) * 100\nprint(\"\\n------ Porcentaje de Clases en la Variable Objetivo ------\")\nprint(f\"Clase 'no': {target_percentage['no']:.2f}%\")\nprint(f\"Clase 'yes': {target_percentage['yes']:.2f}%\")\n\n# 3. Visualización de la distribución de la variable objetivo.\nplt.figure(figsize=(8, 6))\nsns.countplot(x='y', data=df, palette=['#34495e', '#2ecc71'])\nplt.title('Distribución de la Variable Objetivo (y)', fontsize=16)\nplt.xlabel('Suscripción a Depósito a Plazo', fontsize=12)\nplt.ylabel('Frecuencia Absoluta', fontsize=12)\nplt.xticks([0, 1], ['No', 'Sí'])\n\n# Anotaciones de porcentaje sobre las barras de la gráfica\nfor i, percentage in enumerate(target_percentage):\n    plt.text(i, target_counts.iloc[i] + 500,\n             f'{percentage:.2f}%',\n             ha='center', va='center', fontsize=14, color='black')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ca2e35fd","cell_type":"markdown","source":"\n\nLos resultados numéricos y la visualización gráfica confirman que el conjunto de datos presenta un **marcado desbalance de clases**.\n\n* **Clase Mayoritaria (`'no'`):** Corresponde al **88.73%** de las instancias.\n* **Clase Minoritaria (`'yes'`):** Representa únicamente el **11.27%** de las instancias.\n\n#### **Implicaciones del Desbalance de Clases para el Modelado**\n\nEste desbalance es un factor crítico que debe ser considerado durante el desarrollo y la evaluación del modelo por las siguientes razones:\n\n1.  **Invalidez de la Métrica de Exactitud (`Accuracy`):** Un modelo que clasifique todas las instancias como pertenecientes a la clase mayoritaria (`'no'`) alcanzaría una exactitud del 88.73%. Aunque numéricamente alto, este modelo carecería de toda utilidad práctica, pues su objetivo es, precisamente, identificar a la clase minoritaria. Por lo tanto, la exactitud no es una métrica de evaluación fiable en este contexto.\n\n2.  **Sesgo del Modelo Durante el Entrenamiento:** Los algoritmos de aprendizaje, incluyendo las redes neuronales, tienden a optimizar sus parámetros para minimizar una función de pérdida global. En un escenario desbalanceado, el modelo puede lograr una baja pérdida simplemente al aprender a clasificar correctamente la clase mayoritaria, ignorando los patrones distintivos de la clase minoritaria.\n\n3.  **Requerimiento de Métricas de Evaluación Alternativas:** Para una evaluación de rendimiento robusta, es necesario emplear métricas que sean sensibles al desempeño en la clase minoritaria. Las métricas adecuadas para este problema son:\n    * **Precisión (`Precision`):** Mide la proporción de predicciones positivas que fueron correctas. Es fundamental para asegurar la eficiencia de las campañas.\n    * **Recall (Sensibilidad o `Recall`):** Mide la proporción de positivos reales que fueron identificados correctamente. Es vital para maximizar la captación de clientes.\n    * **Puntuación F1 (`F1-Score`):** La media armónica de Precisión y Recall, que proporciona una medida de rendimiento balanceada.\n    * **Área Bajo la Curva ROC (AUC-ROC):** Evalúa la capacidad del modelo para discriminar entre las clases positiva y negativa.\n\nEl reconocimiento temprano de este desbalance condiciona la estrategia de modelado y, fundamentalmente, el marco de evaluación del rendimiento del clasificador final.","metadata":{}},{"id":"44c1b45e","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 2.3.1: Análisis de la variable numérica 'age'.\n\nVisualizaremos la distribución de la edad para cada una de las clases\nde la variable objetivo.\n\"\"\"\n\nplt.figure(figsize=(12, 7))\n\n# Gráfico de densidad para clientes que dijeron 'no'\nsns.kdeplot(df.loc[df['y'] == 'no', 'age'], \n            label='No Suscribió (y=no)', fill=True, color='#34495e')\n\n# Gráfico de densidad para clientes que dijeron 'sí'\nsns.kdeplot(df.loc[df['y'] == 'yes', 'age'], \n            label='Sí Suscribió (y=yes)', fill=True, color='#2ecc71')\n\nplt.title('Distribución de Edad por Resultado de la Campaña', fontsize=16)\nplt.xlabel('Edad del Cliente', fontsize=12)\nplt.ylabel('Densidad', fontsize=12)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"716891fa","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 2.3.2: Análisis de la variable categórica 'job'.\n\nSe calculará y graficará la tasa de conversión para cada tipo de trabajo.\n\"\"\"\n\n# Calcular la tasa de conversión por 'job'\n# Agrupamos por 'job', calculamos la media de una versión numérica de 'y'\nconversion_rate_job = df.groupby('job')['y'].apply(lambda x: (x == 'yes').mean()).sort_values(ascending=False)\n\n# Crear la gráfica\nplt.figure(figsize=(14, 8))\nax = sns.barplot(x=conversion_rate_job.index, y=conversion_rate_job.values * 100, palette='viridis')\n\nplt.title('Tasa de Conversión (%) por Tipo de Trabajo', fontsize=16)\nplt.xlabel('Tipo de Trabajo', fontsize=12)\nplt.ylabel('Tasa de Conversión (%)', fontsize=12)\nplt.xticks(rotation=45, ha='right') # Rotar etiquetas para mejorar legibilidad\n\n# Añadir anotaciones de porcentaje en las barras\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n                textcoords='offset points')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3cd1ee2a","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 2.3.3: Análisis de la variable temporal 'month'.\n\nSe calculará y graficará la tasa de conversión para cada mes.\n\"\"\"\n\n# Orden de los meses para una visualización lógica\nmonth_order = ['mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n\n# Calcular la tasa de conversión por 'month'\nconversion_rate_month = df.groupby('month')['y'].apply(lambda x: (x == 'yes').mean()).reindex(month_order)\n\n# Crear la gráfica\nplt.figure(figsize=(12, 7))\nax = sns.barplot(x=conversion_rate_month.index, y=conversion_rate_month.values * 100, palette='plasma')\n\nplt.title('Tasa de Conversión (%) por Mes', fontsize=16)\nplt.xlabel('Mes del Último Contacto', fontsize=12)\nplt.ylabel('Tasa de Conversión (%)', fontsize=12)\n\n# Añadir anotaciones de porcentaje\nfor p in ax.patches:\n    if p.get_height() > 0: # Solo anotar si hay valor\n        ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),\n                    ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n                    textcoords='offset points')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"18a4b76f","cell_type":"markdown","source":"## 3. Preprocesamiento de Datos para el Modelo de Deep Learning\n\nLas redes neuronales, en su forma más común, operan exclusivamente con datos numéricos. Nuestro conjunto de datos, sin embargo, contiene una mezcla de variables numéricas y categóricas. Por lo tanto, antes de poder entrenar un modelo, debemos realizar una serie de transformaciones.\n\nEl preprocesamiento de datos para este proyecto se centrará en tres tareas principales:\n\n1.  **Codificación de la Variable Objetivo:** Convertir la variable `y` ('yes'/'no') a un formato numérico (1/0).\n2.  **Codificación de Variables Categóricas:** Transformar las variables de texto (como `job` o `marital`) en una representación numérica que el modelo pueda interpretar.\n3.  **Escalado de Características Numéricas:** Estandarizar las variables numéricas (como `age` o `duration`) para que tengan una media de 0 y una desviación estándar de 1. Esto es crucial para la correcta convergencia de los algoritmos de optimización como el descenso de gradiente.\n","metadata":{}},{"id":"c2297dd2","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 3.1: Pipeline de preprocesamiento de datos.\n\nSe aplicarán técnicas de codificación y escalado para preparar los datos\npara el entrenamiento de la red neuronal.\n\"\"\"\n\n# 1. Importar las herramientas necesarias de scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# 2. Separar las características (X) y la variable objetivo (y) del DataFrame original\nX = df.drop('y', axis=1)\ny = df['y']\n\n# 3. Codificar la variable objetivo 'y' a formato numérico (0 y 1)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n# 'no' se codifica como 0, 'yes' como 1.\nprint(f\"Clases de la variable objetivo: {label_encoder.classes_}\")\nprint(f\"Primeras 10 etiquetas codificadas: {y_encoded[:10]}\")\n\n\n# 4. Identificar las columnas numéricas y categóricas\n# Considere en este punto que datos deben excluirse \n# A modo de ejemplo mantendremos todas características, \n# pero la seleccion es una discusión importante.\nnumeric_features = X.select_dtypes(include=np.number).columns.tolist()\ncategorical_features = X.select_dtypes(include='object').columns.tolist()\n\nprint(f\"\\nCaracterísticas numéricas ({len(numeric_features)}): {numeric_features}\")\nprint(f\"Características categóricas ({len(categorical_features)}): {categorical_features}\")\n\n\n# 5. Crear el pipeline de preprocesamiento\n# Un pipeline encapsula una secuencia de transformaciones.\n\n# Pipeline para características numéricas: solo se necesita escalado.\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\n# Pipeline para características categóricas: se necesita codificación One-Hot.\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n    # handle_unknown='ignore' evita errores si aparecen nuevas categorías en test.\n    # drop='first' evita multicolinealidad al eliminar una categoría por variable.\n])\n\n# 6. Unir los pipelines con ColumnTransformer\n# ColumnTransformer aplica diferentes transformaciones a diferentes columnas.\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough' # Mantiene columnas no especificadas (si las hubiera)\n)\n\n# 7. Dividir los datos en conjuntos de entrenamiento y prueba\n# Se reserva un 20% de los datos para la evaluación final del modelo.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n    # stratify=y_encoded asegura que la proporción de clases sea la misma\n    # en los conjuntos de entrenamiento y prueba, lo cual es vital.\n)\n\n# 8. Aplicar el preprocesamiento\n# .fit_transform() en los datos de entrenamiento para aprender los parámetros\n# de escalado y codificación.\nX_train_processed = preprocessor.fit_transform(X_train)\n\n# .transform() en los datos de prueba usando los parámetros aprendidos del\n# conjunto de entrenamiento. Esto evita la fuga de datos (data leakage).\nX_test_processed = preprocessor.transform(X_test)\n\n\nprint(f\"\\nDimensiones de los datos de entrenamiento procesados: {X_train_processed.shape}\")\nprint(f\"Dimensiones de los datos de prueba procesados: {X_test_processed.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1c57399a","cell_type":"markdown","source":"\n\nLa preparación de los datos para el modelo de red neuronal se realizó siguiendo una estrategia estructurada, cuyas decisiones se justifican a continuación.\n\n#### **Manejo de Variables Categóricas**\n\nPara las variables categóricas (`job`, `marital`, `education`, etc.), se seleccionó la técnica de **Codificación One-Hot (`OneHotEncoder`)**.\n\n* **Justificación:** Esta técnica crea una nueva columna binaria (0 o 1) para cada categoría dentro de una variable. Se prefiere sobre otras técnicas como la Codificación de Etiquetas (`LabelEncoder`) para variables nominales (donde las categorías no tienen un orden intrínseco) por una razón fundamental:\n    * **Evita la Creación de un Orden Artificial:** `LabelEncoder` asignaría a cada categoría un entero (e.g., `student`=1, `retired`=2, `services`=3). El modelo podría interpretar erróneamente que existe una relación ordinal (e.g., que `services` > `retired`), lo cual es incorrecto y podría introducir un sesgo no deseado. `OneHotEncoder` trata cada categoría como una entidad independiente, eliminando este riesgo.\n    * Se utilizó el parámetro `drop='first'` para eliminar una de las columnas generadas por cada variable, evitando así la multicolinealidad perfecta, lo cual es una buena práctica en modelado.\n\n#### **Manejo de Variables Numéricas**\n\nPara las variables numéricas (`age`, `campaign`, `euribor3m`, etc.), se aplicó la **Estandarización (`StandardScaler`)**.\n\n* **Justificación:** La estandarización transforma los datos para que tengan una **media de 0 y una desviación estándar de 1**. Esta técnica es crucial para el entrenamiento de redes neuronales por dos motivos principales:\n    * **Convergencia del Optimizador:** Los algoritmos de optimización basados en gradiente, como *Adam* o *SGD*, convergen mucho más rápido cuando las características se encuentran en una escala similar. Si una característica tiene un rango de valores mucho mayor que otras (e.g., `pdays` vs. `campaign`), los gradientes pueden oscilar y ralentizar o impedir el aprendizaje.\n    * **Igualdad de Contribución Inicial:** La estandarización asegura que todas las características tengan el mismo \"peso\" inicial en el cálculo de la función de pérdida, permitiendo que el modelo aprenda su importancia real a través de los pesos sinápticos de manera más efectiva.\n\n#### **División de Datos Estratificada**\n\nFinalmente, el conjunto de datos se dividió en un 80% para entrenamiento y un 20% para prueba, utilizando una **división estratificada (`stratify=y_encoded`)**.\n\n* **Justificación:** Dado el severo desbalance de clases (89% vs. 11%), una división aleatoria simple podría resultar en una proporción de clases significativamente diferente entre los conjuntos de entrenamiento y prueba. La estratificación garantiza que esta proporción se mantenga constante en ambas divisiones, lo que permite una evaluación del modelo mucho más fiable y representativa del problema original.\n\n#### El Impacto de la Fuga de Datos con la Variable 'duration'\n* **Contexto**: La fuga de datos (data leakage) ocurre cuando se utiliza información en el entrenamiento del modelo que no estaría disponible en un escenario de predicción real. La variable duration (duración de la llamada) es un ejemplo clásico: una llamada larga a menudo se correlaciona con el interés del cliente, pero no conocemos su duración antes de realizarla.\n\n    * Crea un nuevo pipeline de preprocesamiento que excluya la variable duration.\n\n    * Entrena el mismo modelo base con este nuevo conjunto de datos.\n\n    * Compara la métrica AUC del modelo entrenado con duration versus el modelo entrenado sin duration.\n\n```\n# --- Experimento: Excluir 'duration' ---\nprint(\"--- Creando conjunto de datos sin la variable 'duration' ---\")\n\n# Excluir 'duration' de las características numéricas\nnumeric_features_no_duration = [feat for feat in numeric_features if feat != 'duration']\n\n# Crear un nuevo preprocesador\npreprocessor_no_duration = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features_no_duration),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# Aplicar el nuevo preprocesamiento\nX_train_no_duration = preprocessor_no_duration.fit_transform(X_train)\nX_test_no_duration = preprocessor_no_duration.transform(X_test)\n\nprint(f\"Dimensiones de los datos sin 'duration': {X_train_no_duration.shape}\")\n```","metadata":{}},{"id":"c0567568","cell_type":"markdown","source":"## 4. Diseño y Entrenamiento del Modelo de Deep Learning\n\nCon los datos ya procesados, procederemos a construir la arquitectura de la red neuronal. El diseño de la arquitectura es un paso fundamental que implica definir el número de capas, la cantidad de neuronas en cada capa, las funciones de activación y las técnicas para prevenir el sobreajuste.\n\nPosteriormente, compilaremos el modelo especificando el optimizador y la función de pérdida, y lo entrenaremos con nuestros datos. Esta sección responderá a las **Preguntas 4 y 5**.","metadata":{}},{"id":"a178b9dd","cell_type":"markdown","source":"#### **Tarea: Definir la Arquitectura de su Modelo**\n\nUtilizando la función `build_classifier` definida más abajo, su tarea es experimentar y definir la arquitectura de la red neuronal. Modifique los valores en la siguiente celda de código para explorar cómo los cambios afectan el rendimiento del modelo.\n\n**Parámetros a experimentar:**\n* `HIDDEN_LAYERS`: Pruebe con más o menos capas, y con diferente número de neuronas (e.g., `(128, 64, 32)`, `(32,)`, `(100, 50)`).\n* `DROPOUT_RATE`: Varíe la tasa de dropout (e.g., `0.2`, `0.5`) o desactívelo (`0.0`).\n* `LEARNING_RATE`: Pruebe con tasas de aprendizaje más altas o más bajas (e.g., `0.01`, `0.0001`).\n\nUna vez que haya decidido una arquitectura final, utilice el resumen del modelo (`model.summary()`) y la justificación de la celda siguiente para responder a la **Pregunta 4**.","metadata":{}},{"id":"176babbb","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 4.1: Función reutilizable para construir el modelo clasificador.\n\nSe define una función que encapsula la lógica de construcción y compilación\nde la red neuronal, permitiendo modificar su arquitectura fácilmente a través\nde parámetros.\n\"\"\"\n\n# 1. Importar las clases y funciones necesarias de TensorFlow y Keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\ndef build_classifier(n_features, hidden_layers=(64, 32),\n                     activation='relu', dropout_rate=0.3,\n                     learning_rate=0.001):\n    \"\"\"Construye y compila un modelo de red neuronal secuencial de Keras.\n\n    Esta función crea un clasificador binario con una arquitectura flexible.\n    La estructura consiste en una capa de entrada, un número variable de capas\n    ocultas densas (con Dropout opcional) y una capa de salida sigmoide.\n\n    Args:\n        n_features (int):\n            El número de características de entrada del modelo. Corresponde\n            a la dimensionalidad de los datos de entrada (shape[1]).\n        hidden_layers (tuple of int, optional):\n            Una tupla donde cada entero representa el número de neuronas en una\n            capa oculta. El número de elementos en la tupla determina el\n            número de capas ocultas.\n            Defaults to (64, 32).\n        activation (str, optional):\n            La función de activación a utilizar en las capas ocultas.\n            Defaults to 'relu'.\n        dropout_rate (float, optional):\n            La tasa de Dropout a aplicar después de cada capa oculta. Debe ser\n            un valor entre 0.0 y 1.0. Si es 0, no se aplicará Dropout.\n            Defaults to 0.3.\n        learning_rate (float, optional):\n            La tasa de aprendizaje para el optimizador Adam.\n            Defaults to 0.001.\n\n    Returns:\n        tensorflow.keras.models.Sequential:\n            Un modelo de Keras compilado y listo para ser entrenado.\n    \"\"\"\n    # Inicializar el modelo secuencial, que es un stack lineal de capas.\n    model = Sequential(name=\"Bank_Marketing_Classifier\")\n\n    # --- Capa de Entrada ---\n    # La primera capa Dense también define la capa de entrada a través del\n    # argumento `input_shape`. Se añade automáticamente.\n    model.add(Dense(\n        units=hidden_layers[0],\n        activation=activation,\n        input_shape=(n_features,),\n        name=f\"Capa_Oculta_1_{hidden_layers[0]}_neuronas\"\n    ))\n    \n    # Aplicar Dropout si la tasa es mayor que cero.\n    if dropout_rate > 0:\n        model.add(Dropout(dropout_rate, name=\"Dropout_1\"))\n\n    # --- Capas Ocultas Adicionales ---\n    # Iterar sobre el resto de las capas definidas en la tupla `hidden_layers`.\n    # Se empieza desde el segundo elemento (índice 1).\n    for i, neurons in enumerate(hidden_layers[1:], start=2):\n        model.add(Dense(\n            units=neurons,\n            activation=activation,\n            name=f\"Capa_Oculta_{i}_{neurons}_neuronas\"\n        ))\n        if dropout_rate > 0:\n            model.add(Dropout(dropout_rate, name=f\"Dropout_{i}\"))\n\n    # --- Capa de Salida ---\n    # Para clasificación binaria, se usa una única neurona con activación sigmoide.\n    # La salida será una probabilidad (un valor entre 0 y 1).\n    # Esto es una sugerencia pero puede modificarlo y probar otras alternativas.\n    model.add(Dense(1, activation='sigmoid', name=\"Capa_Salida_Sigmoide\"))\n\n    # --- Compilación del Modelo ---\n    # Crear una instancia del optimizador Adam con la tasa de aprendizaje definida.\n    optimizer = Adam(learning_rate=learning_rate)\n    \n    # Configurar el proceso de aprendizaje del modelo.\n    model.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy', # Función de pérdida para clasificación binaria.\n        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n    )\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9584128b","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 4.2: Configuración de hiperparámetros y construcción del modelo.\n\nLos estudiantes deben modificar estas variables para definir su arquitectura.\n\"\"\"\n\n# --- Panel de Control de la Arquitectura ---\n# Modifique estos valores para experimentar.\n\n# Tupla con las neuronas de cada capa oculta.\n# Ejemplo: (64, 32) -> Dos capas ocultas, la primera con 64 neuronas, la segunda con 32.\nHIDDEN_LAYERS = (32,)\n\n# Tasa de Dropout (0.0 para desactivar).\nDROPOUT_RATE = 0.0\n\n# Tasa de aprendizaje para el optimizador Adam.\nLEARNING_RATE = 0.1\n# ---------------------------------------------\n\n\n# Obtener el número de características del conjunto de datos procesado.\nn_features = X_train_processed.shape[1]\n\n# Llamar a la función para construir el modelo con los parámetros definidos.\nmodel = build_classifier(\n    n_features,\n    hidden_layers=HIDDEN_LAYERS,\n    dropout_rate=DROPOUT_RATE,\n    learning_rate=LEARNING_RATE\n)\n\n# Imprimir el resumen de la arquitectura del modelo final.\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"01c90ca3-29a1-4148-9cbe-89879c92f213","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 4.3: Entrenamiento del modelo y aplicación de técnicas contra el sobreajuste.\n\nSe entrena el modelo compilado utilizando los datos de entrenamiento y se\nimplementa EarlyStopping para finalizar el proceso de forma óptima.\n\"\"\"\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.utils import class_weight\nimport numpy as np\n\n# Calcula los pesos de clase para que sean inversamente proporcionales a su frecuencia.\n# La clase minoritaria ('yes', codificada como 1) recibirá un peso mucho mayor.\nweights = class_weight.compute_class_weight(\n    'balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\n\n# El resultado será algo como: array([ 0.56, 4.43])\n# Significa que un error en la clase '1' ('yes') penalizará ~8 veces más\n# que un error en la clase '0' ('no').\nclass_weights = {i : weights[i] for i,_ in enumerate(weights)}\n\nprint(\"Pesos de Clase Calculados:\")\nprint(class_weights)\n\n# 1. Definir el callback de EarlyStopping para prevenir el sobreajuste.\n# Esta técnica monitorea una métrica de interés (la pérdida en el conjunto de\n# validación, 'val_loss') y detiene el entrenamiento si no se observa una\n# mejora tras un número definido de épocas ('patience').\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss', # Métrica a monitorear.\n    patience=10,        # Nº de épocas a esperar sin mejora antes de parar.\n    verbose=1,          # Informar en consola cuando el entrenamiento se detiene.\n    mode='min',         # La monitorización se detiene cuando la métrica deja de disminuir.\n    restore_best_weights=True # Restaura los pesos del modelo de la mejor época.\n)\n\n# 2. Entrenar el modelo con el método .fit().\n# Se proporciona el conjunto de datos de entrenamiento (X_train_processed, y_train).\n# El entrenamiento se ejecutará por un máximo de 100 épocas, pero EarlyStopping\n# probablemente lo detendrá antes.\n\nprint(\"\\n------ Iniciando Entrenamiento del Modelo ------\")\nhistory = model.fit(\n    X_train_processed,\n    y_train,\n    epochs=100,             # Número máximo de épocas.\n    batch_size=512,          # Número de muestras por actualización de gradiente.\n    validation_split=0.2,   # Porcentaje de datos de entrenamiento a usar para validación.\n    callbacks=[early_stopping], # Lista de callbacks a aplicar durante el entrenamiento.\n    verbose=1               # Muestra una barra de progreso.\n)\nprint(\"------ Entrenamiento Finalizado ------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7b07474b-851b-46bc-bae7-74db2f218a65","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nVisualización del rendimiento durante el entrenamiento.\n\nGraficar las curvas de aprendizaje (pérdida y métricas) para los conjuntos de\nentrenamiento y validación es esencial para diagnosticar el sobreajuste y\nevaluar el proceso de aprendizaje del modelo.\n\"\"\"\n# El objeto 'history' devuelto por model.fit() contiene un diccionario\n# con los valores de pérdida y métricas de cada época.\n# Lo convertimos a un DataFrame de pandas para facilitar la manipulación.\nhistory_df = pd.DataFrame(history.history)\n\n# Creamos una figura con dos subplots, uno al lado del otro.\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# --- Gráfico de la Función de Pérdida (Loss) ---\n# Compara cómo evolucionó el error en los datos de entrenamiento vs. validación.\naxes[0].plot(history_df['loss'], label='Pérdida de Entrenamiento', color='#3498db', lw=2)\naxes[0].plot(history_df['val_loss'], label='Pérdida de Validación', color='#e74c3c', lw=2, linestyle='--')\naxes[0].set_title('Curvas de Aprendizaje: Pérdida', fontsize=16)\naxes[0].set_xlabel('Épocas', fontsize=12)\naxes[0].set_ylabel('Binary Cross-Entropy', fontsize=12)\naxes[0].legend()\naxes[0].grid(True)\n\n# --- Gráfico de la Métrica de Rendimiento (AUC) ---\n# Compara el rendimiento (AUC) en los datos de entrenamiento vs. validación.\n# Usamos AUC en lugar de Accuracy por ser más robusto al desbalance.\naxes[1].plot(history_df['auc'], label='AUC de Entrenamiento', color='#3498db', lw=2)\naxes[1].plot(history_df['val_auc'], label='AUC de Validación', color='#e74c3c', lw=2, linestyle='--')\naxes[1].set_title('Curvas de Aprendizaje: AUC', fontsize=16)\naxes[1].set_xlabel('Épocas', fontsize=12)\naxes[1].set_ylabel('AUC', fontsize=12)\naxes[1].legend()\naxes[1].grid(True)\n\n# Ajusta el layout para evitar solapamientos y muestra la figura.\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3a8b85b1","cell_type":"markdown","source":"## 5. Evaluación del Rendimiento del Modelo\n\nHa llegado el momento de la verdad. Hasta ahora, hemos entrenado el modelo y monitoreado su rendimiento utilizando un conjunto de validación derivado de los datos de entrenamiento. Ahora, evaluaremos su capacidad de generalización final utilizando el **conjunto de prueba (`test set`)**, que el modelo no ha visto en ninguna etapa anterior.\n\nEsta evaluación nos permitirá obtener una estimación imparcial de cómo se comportaría el modelo en un entorno de producción con datos nuevos. Nos centraremos en las métricas de clasificación clave para abordar la **Pregunta 6**.","metadata":{}},{"id":"bb7e4eab","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 5.1: Evaluación del modelo en el conjunto de prueba.\n\nSe calculan las métricas de rendimiento y la matriz de confusión.\n\"\"\"\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport seaborn as sns\n\n# 1. Evaluar el modelo usando el método .evaluate() de Keras\n# Este método devuelve la pérdida (loss) y las métricas que definimos\n# al compilar el modelo (accuracy, auc).\nprint(\"------ Evaluación General de Keras ------\")\nloss, accuracy, auc_score = model.evaluate(X_test_processed, y_test, verbose=0)\nprint(f\"Pérdida en el conjunto de prueba: {loss:.4f}\")\nprint(f\"Exactitud (Accuracy) en el conjunto de prueba: {accuracy:.4f}\")\nprint(f\"Área Bajo la Curva ROC (AUC) en el conjunto de prueba: {auc_score:.4f}\")\n\n\n# 2. Realizar predicciones de probabilidad\n# El método .predict() devuelve la salida de la capa sigmoide.\ny_pred_proba = model.predict(X_test_processed).flatten()\n\n# 3. Convertir probabilidades a clases binarias (0 o 1)\n# Se utiliza un umbral estándar de 0.5.\ny_pred_class = (y_pred_proba > 0.5).astype(int)\n\n\n# 4. Generar el reporte de clasificación de scikit-learn\n# Este reporte incluye Precisión, Recall y F1-Score.\nprint(\"\\n------ Reporte de Clasificación Detallado ------\")\nprint(classification_report(y_test, y_pred_class, target_names=['No', 'Sí']))\n\n\n# 5. Generar y visualizar la Matriz de Confusión\nprint(\"\\n------ Matriz de Confusión ------\")\ncm = confusion_matrix(y_test, y_pred_class)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Predicho No', 'Predicho Sí'],\n            yticklabels=['Real No', 'Real Sí'])\nplt.title('Matriz de Confusión', fontsize=16)\nplt.ylabel('Etiqueta Real', fontsize=12)\nplt.xlabel('Etiqueta Predicha', fontsize=12)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b91d48a4","cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nPaso 5.2: Visualización de la Curva PR.\n\nEsta curva muestra el rendimiento de un modelo de clasificación en todos\nlos umbrales de clasificación.\n\"\"\"\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n\nplt.figure(figsize=(10, 7))\nplt.plot(recall, precision, color='purple', lw=2)\nplt.xlabel('Recall (Sensibilidad)')\nplt.ylabel('Precisión')\nplt.title('Curva de Precisión-Recall')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}